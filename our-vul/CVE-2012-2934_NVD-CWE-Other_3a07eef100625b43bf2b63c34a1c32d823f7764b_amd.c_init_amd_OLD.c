static void __devinit init_amd(struct cpuinfo_x86 *c)
{
	u32 l, h;

	unsigned long long value;

	





	if (c->x86 == 15) {
		rdmsrl(MSR_K7_HWCR, value);
		value |= 1 << 6;
		wrmsrl(MSR_K7_HWCR, value);
	}

	





	

	clear_bit(0*32+31, c->x86_capability);
	
#ifdef CONFIG_X86_64
	if (c->x86 == 0xf && c->x86_model < 0x14
	    && cpu_has(c, X86_FEATURE_LAHF_LM)) {
		




		unsigned int lo, hi;

		clear_bit(X86_FEATURE_LAHF_LM, c->x86_capability);
		if (!rdmsr_amd_safe(0xc001100d, &lo, &hi)) {
			hi &= ~1;
			wrmsr_amd_safe(0xc001100d, lo, hi);
		}
	}
#endif

	switch(c->x86)
	{
	case 6: 
 
		



		if (c->x86_model >= 6 && c->x86_model <= 10) {
			if (!cpu_has(c, X86_FEATURE_XMM)) {
				printk(KERN_INFO "Enabling disabled K7/SSE Support.\n");
				rdmsr(MSR_K7_HWCR, l, h);
				l &= ~0x00008000;
				wrmsr(MSR_K7_HWCR, l, h);
				set_bit(X86_FEATURE_XMM, c->x86_capability);
			}
		}

		



		if ((c->x86_model == 8 && c->x86_mask>=1) || (c->x86_model > 8)) {
			rdmsr(MSR_K7_CLK_CTL, l, h);
			if ((l & 0xfff00000) != 0x20000000) {
				printk ("CPU: CLK_CTL MSR was %x. Reprogramming to %x\n", l,
					((l & 0x000fffff)|0x20000000));
				wrmsr(MSR_K7_CLK_CTL, (l & 0x000fffff)|0x20000000, h);
			}
		}
		set_bit(X86_FEATURE_K7, c->x86_capability);
		break;

	case 0xf:
	
	case 0x10 ... 0x17:
		set_bit(X86_FEATURE_K8, c->x86_capability);
		disable_c1e(NULL);
		if (acpi_smi_cmd && (acpi_enable_value | acpi_disable_value))
			pv_post_outb_hook = check_disable_c1e;
		break;
	}

	display_cacheinfo(c);

	if (cpuid_eax(0x80000000) >= 0x80000008) {
		c->x86_max_cores = (cpuid_ecx(0x80000008) & 0xff) + 1;
	}

	if (cpuid_eax(0x80000000) >= 0x80000007) {
		c->x86_power = cpuid_edx(0x80000007);
		if (c->x86_power & (1<<8)) {
			set_bit(X86_FEATURE_CONSTANT_TSC, c->x86_capability);
			set_bit(X86_FEATURE_NONSTOP_TSC, c->x86_capability);
			if (c->x86 != 0x11)
				set_bit(X86_FEATURE_TSC_RELIABLE, c->x86_capability);
		}
	}

	
	if ((c->x86 == 0x15) &&
	    (c->x86_model >= 0x10) && (c->x86_model <= 0x1f) &&
	    !cpu_has(c, X86_FEATURE_TOPOEXT) &&
	    !rdmsr_safe(MSR_K8_EXT_FEATURE_MASK, value)) {
		value |= 1ULL << 54;
		wrmsr_safe(MSR_K8_EXT_FEATURE_MASK, value);
		rdmsrl(MSR_K8_EXT_FEATURE_MASK, value);
		if (value & (1ULL << 54)) {
			set_bit(X86_FEATURE_TOPOEXT, c->x86_capability);
			printk(KERN_INFO "CPU: Re-enabling disabled "
			       "Topology Extensions Support\n");
		}
	}

        amd_get_topology(c);

	
	if (c->x86 >= 0x10 && !force_mwait)
		clear_bit(X86_FEATURE_MWAIT, c->x86_capability);

#ifdef __x86_64__
	
	clear_bit(X86_FEATURE_SEP, c->x86_capability);

	if (c->x86 == 0x10) {
		
		if (c == &boot_cpu_data)
			check_enable_amd_mmconf_dmi();

		fam10h_check_enable_mmcfg();
	}
#endif

	



	if (c->x86 > 0x11)
		set_bit(X86_FEATURE_ARAT, c->x86_capability);

	if (cpuid_edx(0x80000007) & (1 << 10)) {
		rdmsr(MSR_K7_HWCR, l, h);
		l |= (1 << 27); 
		wrmsr(MSR_K7_HWCR, l, h);
	}

	
	if ((smp_processor_id() == 1) && c1_ramping_may_cause_clock_drift(c))
		disable_c1_ramping();

	set_cpuidmask(c);

	check_syscfg_dram_mod_en();
}